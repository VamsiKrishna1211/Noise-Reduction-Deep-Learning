{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from python_files.unet_basic import Model\n",
    "from python_files.Noise_Reduction_Datagen_paths import Signal_Synthesis_DataGen\n",
    "from fastai.text.all import *\n",
    "from fastai.data.core import DataLoaders\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(dir_path):\n",
    "    paths = []\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        for name in files:\n",
    "            if name.endswith(\".wav\") or name.endswith(\".mp3\"):\n",
    "                paths.append(os.path.join(root, name))\n",
    "                \n",
    "    paths = np.asarray(paths)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_paths = np.load(\"./dataset_loader_files/signal_paths_nums_save.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 ./dataset/Reduced_clean_signals/common_voice_en_22252098.mp3\n"
     ]
    }
   ],
   "source": [
    "noise_paths = get_paths(\"./dataset/Reduced_noise/\")[:1000]\n",
    "signal_paths = get_paths(\"./dataset/Reduced_clean_signals/\")[:1000]\n",
    "signal_dir = \"\"#\"./dataset/cv-corpus-5.1-2020-06-22-Resampled/en/clips\"\n",
    "noise_save_path = \"\"#\"./dataset_loader_files/noise_paths_resampled_save.npy\"\n",
    "train = True\n",
    "n_fft=1024\n",
    "win_length=n_fft\n",
    "hop_len=n_fft//4\n",
    "create_specgram = False\n",
    "perform_stft = False\n",
    "default_sr = 16000\n",
    "sec = (16384/default_sr)*2\n",
    "augment=True\n",
    "device_datagen = \"cpu\"\n",
    "\n",
    "train_ds = Signal_Synthesis_DataGen(noise_paths, signal_paths, signal_dir, \\\n",
    "                 n_fft=n_fft, win_length=win_length, hop_len=hop_len, create_specgram=create_specgram, \\\n",
    "                 perform_stft=perform_stft, normalize=True, default_sr=default_sr, sec=sec, epsilon=1e-5, augment=False, device=device_datagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ./dataset/Reduced_clean_signals/common_voice_en_22347670.mp3\n"
     ]
    }
   ],
   "source": [
    "noise_paths = get_paths(\"./dataset/Reduced_noise/\")[1200:1300]\n",
    "signal_paths = get_paths(\"./dataset/Reduced_clean_signals/\")[6000:6100]\n",
    "signal_dir = \"\"#\"./dataset/cv-corpus-5.1-2020-06-22-Resampled/en/clips\"\n",
    "noise_save_path = \"\"#\"./dataset_loader_files/noise_paths_resampled_save.npy\"\n",
    "train = False\n",
    "\n",
    "val_ds = Signal_Synthesis_DataGen(noise_paths, signal_paths, signal_dir,\\\n",
    "                 n_fft=n_fft, win_length=win_length, hop_len=hop_len, create_specgram=create_specgram, \\\n",
    "                 perform_stft=perform_stft, normalize=True, default_sr=default_sr, sec=sec, epsilon=1e-5, augment=False, device=device_datagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BATCH_SIZE = 400\n",
    "shuffle = True\n",
    "num_workers = 0\n",
    "pin_memory = False\n",
    "\n",
    "# data_loader = DataLoader(signal_synthesis_dataset, batch_size=BATCH_SIZE, shuffle=shuffle, num_workers=num_workers)\n",
    "# data_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=shuffle, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "dls = DataLoaders.from_dsets(train_ds, val_ds, bs=BATCH_SIZE, num_workers=num_workers, pin_memory=pin_memory).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for data in dls.train:\n",
    "#     print(data[0].max(), data[0].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mod_MSELoss(nn.Module):\n",
    "    def __init__(self, mul_factor):\n",
    "        super(Mod_MSELoss, self).__init__()\n",
    "        self.loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "        self.mul_factor = mul_factor\n",
    "        \n",
    "        \n",
    "    def forward(self, sig_pred, sig_true):\n",
    "        loss = self.loss_fn(sig_pred, sig_true)\n",
    "        loss = self.mul_factor*loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSamplingLayer(nn.Module):\n",
    "    def __init__(self, channel_in, channel_out, dilation=1, kernel_size=15, stride=1, padding=7):\n",
    "        super(DownSamplingLayer, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv1d(channel_in, channel_out, kernel_size=kernel_size,\n",
    "                      stride=stride, padding=padding, dilation=dilation),\n",
    "            nn.BatchNorm1d(channel_out),\n",
    "            nn.LeakyReLU(negative_slope=0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, ipt):\n",
    "        return self.main(ipt)\n",
    "\n",
    "class UpSamplingLayer(nn.Module):\n",
    "    def __init__(self, channel_in, channel_out, kernel_size=5, stride=1, padding=2):\n",
    "        super(UpSamplingLayer, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv1d(channel_in, channel_out, kernel_size=kernel_size,\n",
    "                      stride=stride, padding=padding),\n",
    "            nn.BatchNorm1d(channel_out),\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, ipt):\n",
    "        return self.main(ipt)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_layers=12, channels_interval=24):\n",
    "        super(Model, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.channels_interval = channels_interval\n",
    "        encoder_in_channels_list = [1] + [i * self.channels_interval for i in range(1, self.n_layers)]\n",
    "        encoder_out_channels_list = [i * self.channels_interval for i in range(1, self.n_layers + 1)]\n",
    "        range_list = [i for i in range(self.n_layers)]\n",
    "        #          1    => 2    => 3    => 4    => 5    => 6   => 7   => 8   => 9  => 10 => 11 =>12\n",
    "        # 16384 => 8192 => 4096 => 2048 => 1024 => 512 => 256 => 128 => 64 => 32 => 16 =>  8 => 4\n",
    "        self.encoder = nn.ModuleList()\n",
    "        for i in range(self.n_layers):\n",
    "            self.encoder.append(\n",
    "                DownSamplingLayer(\n",
    "                    channel_in=encoder_in_channels_list[i],\n",
    "                    channel_out=encoder_out_channels_list[i]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv1d(self.n_layers * self.channels_interval, self.n_layers * self.channels_interval, 15, stride=1,\n",
    "                      padding=7),\n",
    "            nn.BatchNorm1d(self.n_layers * self.channels_interval),\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        )\n",
    "\n",
    "        decoder_in_channels_list = [(2 * i + 1) * self.channels_interval for i in range(1, self.n_layers)] + [\n",
    "            2 * self.n_layers * self.channels_interval]\n",
    "        decoder_in_channels_list = decoder_in_channels_list[::-1]\n",
    "        decoder_out_channels_list = encoder_out_channels_list[::-1]\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for i in range(self.n_layers):\n",
    "            self.decoder.append(\n",
    "                UpSamplingLayer(\n",
    "                    channel_in=decoder_in_channels_list[i],\n",
    "                    channel_out=decoder_out_channels_list[i]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv1d(1 + self.channels_interval, 1, kernel_size=1, stride=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        tmp = []\n",
    "        o = input\n",
    "        for i, _ in enumerate(self.encoder.children()):\n",
    "            o = self.encoder[i](o)\n",
    "            tmp.append(o)\n",
    "            o = o[:, :, ::2]\n",
    "\n",
    "        o = self.middle(o)\n",
    "\n",
    "        for i, _ in enumerate(self.decoder.children()):\n",
    "            o = F.interpolate(o, scale_factor=2, mode=\"linear\", align_corners=True)\n",
    "            o = torch.cat([o, tmp[self.n_layers - i -1]], dim=1)\n",
    "            o = self.decoder[i](o)\n",
    "\n",
    "        o = torch.cat([o, input], dim=1)\n",
    "        o = self.out(o)\n",
    "        return o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Model(12, 24)\n",
    "model.load_state_dict(torch.load(\"./Model_saves/torch_model_save_large_dataset.pt\"))\n",
    "model.to(\"cpu\")\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0)\n",
    "criterion = Mod_MSELoss(mul_factor=1000)\n",
    "n_epochs=100\n",
    "\n",
    "\n",
    "model.train()\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"./runs/CNN_model_experiment_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.6 s, sys: 144 ms, total: 11.8 s\n",
      "Wall time: 2.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for noise, sig in dls.train:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(model, noise)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_audio(\"runs/CNN_model_experiment_1\", noise[0].squeeze().t().to(\"cpu\").numpy(), sample_rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
