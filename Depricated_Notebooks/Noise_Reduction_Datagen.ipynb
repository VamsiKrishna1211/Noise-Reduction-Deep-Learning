{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio as ta\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import IPython.display as ipd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "\n",
    "from random import shuffle\n",
    "import gc\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Signal_Synthesis_DataGen(Dataset):\n",
    "    def __init__(self, noise_dir, signal_dir, signal_nums_save=None, num_noise_samples=None, num_signal_samples=None, noise_path_save=None,\\\n",
    "                 n_fft=400, win_length=400, hop_len=200, create_specgram=False, \\\n",
    "                 perform_stft=True, normalize=True, default_sr=16000, sec=6, augment=False):\n",
    "\n",
    "        self.noise_dir = noise_dir\n",
    "        self.signal_dir = signal_dir\n",
    "        self.signal_nums_save = signal_nums_save\n",
    "        self.num_noise_samples = num_noise_samples\n",
    "        self.num_signal_samples = num_signal_samples\n",
    "        self.noise_path_save = noise_path_save\n",
    "        self.n_fft = n_fft\n",
    "        self.win_length = win_length\n",
    "        self.hop_len = hop_len\n",
    "        self.create_specgram = create_specgram\n",
    "        self.perform_stft = perform_stft\n",
    "        self.normalize = normalize\n",
    "        self.default_sr = default_sr\n",
    "        self.sec = sec\n",
    "        self.augment = augment\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        if self.create_specgram == True and self.perform_stft == True:\n",
    "            raise Exception(\"Use only one option out of 'create_specgram' and 'perform_stft'\")\n",
    "\n",
    "\n",
    "        if os.path.exists(self.noise_path_save):\n",
    "            print(\"Loading noise from saved file\")\n",
    "            noise_paths = np.load(self.noise_path_save)\n",
    "        else:\n",
    "            noise_paths = []\n",
    "            for root, dirs, files in os.walk(noise_dir):\n",
    "                for name in files:\n",
    "                    if name.endswith(\".wav\"):\n",
    "                        noise_paths.append(os.path.join(root, name))\n",
    "            noise_paths = np.asarray(noise_paths)\n",
    "\n",
    "#         shuffle(noise_paths)\n",
    "        # print(self.num_noise_samples)\n",
    "        if self.num_noise_samples is not None:\n",
    "            self.noise_paths = noise_paths[:self.num_noise_samples]\n",
    "        else:\n",
    "            self.noise_paths = noise_paths\n",
    "        if os.path.exists(signal_nums_save):\n",
    "            print(\"Loading nums from npy file\")\n",
    "            self.signal_nums = torch.from_numpy(np.load(signal_nums_save))\n",
    "        else:\n",
    "            self.signal_nums = torch.from_numpy(self.get_signal_paths(signal_dir))\n",
    "\n",
    "        if self.num_signal_samples is not None:\n",
    "            self.signal_nums = self.signal_nums[:self.num_signal_samples]\n",
    "        print(len(self.signal_nums))\n",
    "        self.prefix = \"common_voice_en_\"\n",
    "        self.suffix = \".mp3\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_signal_paths(self, clips_path):\n",
    "\n",
    "        file_nums = []\n",
    "        for file in tqdm(os.listdir(clips_path)):\n",
    "            num = file.split(\"_\")[3]\n",
    "            num = int(num.split(\".\")[0])\n",
    "            file_nums.append(num)\n",
    "        file_nums = np.asarray(file_nums)\n",
    "        return file_nums\n",
    "\n",
    "\n",
    "\n",
    "    def get_noise_from_sound(self, signal, noise, SNR):\n",
    "\n",
    "        RMS_s = torch.sqrt(torch.mean(signal**2))\n",
    "\n",
    "        RMS_n = torch.sqrt(RMS_s**2/pow(10., SNR/10))\n",
    "\n",
    "        RMS_n_current = torch.sqrt(torch.mean(torch.square(noise)))\n",
    "        noise = noise*(RMS_n/RMS_n_current)\n",
    "\n",
    "        return noise\n",
    "\n",
    "\n",
    "\n",
    "    def get_mixed_signal(self, signal: torch.Tensor, noise: torch.Tensor, default_sr, sec, SNR):\n",
    "\n",
    "        snip_audio = np.random.randint(0, 2)\n",
    "        # if snip_audio:\n",
    "        #     signal = ta.transforms.Vad(sample_rate=default_sr)(signal)\n",
    "\n",
    "        sig_length = int(default_sr * sec)\n",
    "\n",
    "        if len(signal) > sig_length:\n",
    "            signal = signal[: sig_length]\n",
    "        elif len(signal) <= sig_length:\n",
    "            zero_signal = torch.zeros((signal.shape)).to(self.device)\n",
    "            while len(signal) < sig_length:\n",
    "                signal = torch.cat((signal, zero_signal))\n",
    "                zero_signal = torch.zeros(signal.shape).to(self.device)\n",
    "            signal = signal[ : sig_length]\n",
    "\n",
    "\n",
    "        noise_len = len(noise)\n",
    "        signal_len = len(signal)\n",
    "\n",
    "        if len(noise) > len(signal):\n",
    "            noise = noise[0 : len(signal)]\n",
    "        elif len(noise) <= len(signal):\n",
    "\n",
    "            #noise = torch.cat((noise, torch.zeros((len(signal) - len(noise)))))\n",
    "            for i in range(int(len(signal)/len(noise))+1):\n",
    "                noise = torch.cat((noise, noise))\n",
    "\n",
    "            noise = noise[:len(signal)]\n",
    "\n",
    "        noise = self.get_noise_from_sound(signal, noise, SNR)\n",
    "\n",
    "        signal_noise = signal+noise\n",
    "        return signal_noise, signal\n",
    "\n",
    "    def construct_signal_path(self, signal_id):\n",
    "\n",
    "        file_num = self.signal_nums[signal_id]\n",
    "        if torch.is_tensor(file_num):\n",
    "            # print(\"Enter_tensor\")\n",
    "            file_num = file_num.item()\n",
    "        file_name = self.prefix + str(file_num) + self.suffix\n",
    "        path = os.path.join(self.signal_dir, file_name)\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "        else:\n",
    "            raise FileExistsError(f\"{path}\")\n",
    "\n",
    "\n",
    "\n",
    "    def get_ids(self, idx):\n",
    "\n",
    "        signal_id = idx//len(self.noise_paths)\n",
    "        noise_id = idx - signal_id*len(self.noise_paths)\n",
    "#         print(signal_id, noise_id)\n",
    "\n",
    "        signal_path, noise_path = self.construct_signal_path(signal_id), self.noise_paths[noise_id]\n",
    "\n",
    "        signal_noise_add, signal = self.develop_data(signal_path, noise_path)\n",
    "\n",
    "        return signal_noise_add, signal\n",
    "\n",
    "    def develop_data(self, signal_path, noise_path):\n",
    "\n",
    "\n",
    "        SNR = np.random.randint(0, np.random.randint(0, 50)+1)\n",
    "#         print(SNR)\n",
    "\n",
    "        # noise, nsr = librosa.load(noise_path, sr=self.default_sr)\n",
    "        # signal, ssr = librosa.load(signal_path, sr=self.default_sr)\n",
    "        # noise = torch.from_numpy(noise).to(self.device).to(self.device)\n",
    "        # signal = torch.from_numpy(signal).to(self.device).to(self.device)\n",
    "\n",
    "        noise, nsr = ta.load(noise_path)\n",
    "        noise = noise.to(self.device)\n",
    "        noise = noise.type(torch.float32)\n",
    "        if nsr != self.default_sr:\n",
    "            noise = ta.transforms.Resample(orig_freq=nsr, new_freq=self.default_sr)(noise)\n",
    "        signal, ssr = ta.load(signal_path)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = signal.type(torch.float32)\n",
    "        if ssr != self.default_sr:\n",
    "            signal = ta.transforms.Resample(orig_freq=ssr, new_freq=self.default_sr)(signal)\n",
    "\n",
    "        # noise = noise.type(torch.float32)\n",
    "        # signal = signal.type(torch.float32)\n",
    "\n",
    "\n",
    "        signal_noise_add, signal = self.get_mixed_signal(signal, noise, self.default_sr, self.sec, SNR)\n",
    "        if self.perform_stft:\n",
    "            signal_noise_add = torch.stft(signal_noise_add, n_fft=self.n_fft, hop_length=self.hop_len, win_length=self.win_length)\n",
    "            signal = torch.stft(signal, n_fft=self.n_fft, hop_length=self.hop_len, win_length=self.win_length)[:,:,:]\n",
    "            # (signal_noise_add, signal) = torch.stft(combined_signal, n_fft=self.n_fft, hop_length=self.hop_len, win_length=self.win_length, normalized=self.normalize)\n",
    "        elif self.create_specgram:\n",
    "            spec_transformer = ta.transforms.Spectrogram(n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_len, normalized=self.normalize)\n",
    "            signal_noise_add = spec_transformer(signal_noise_add)\n",
    "            signal = spec_transformer(signal)\n",
    "\n",
    "        del signal, noise\n",
    "        gc.collect()\n",
    "\n",
    "        return signal_noise_add, signal\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.signal_nums)*len(self.noise_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        signal_noise_add, signal = self.get_ids(idx)\n",
    "\n",
    "#         signal_noise_add, signal = signal_noise_add/signal_noise_add.max(), signal/signal.max()\n",
    "        # print(\"returning the values from getitem dataset\")\n",
    "        # print(signal.shape)\n",
    "        return signal_noise_add, signal\n",
    "#         return signal_noise_add, signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten1 = torch.randn(46000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten2 = torch.randn(46000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_ten = torch.cat((ten1, ten2), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([92000])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_ten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading noise from saved file\n",
      "Loading nums from npy file\n",
      "1000\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 4.54 GiB (GPU 0; 7.80 GiB total capacity; 4.54 GiB already allocated; 2.38 GiB free; 4.54 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cc1b22368853>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m                      perform_stft=perform_stft, normalize=True, default_sr=default_sr, sec=sec, augment=False)\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0msignal_mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignal_synthesis_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4532\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal_mix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9e14a5df647e>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0msignal_noise_add\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;31m#         signal_noise_add, signal = signal_noise_add/signal_noise_add.max(), signal/signal.max()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9e14a5df647e>\u001b[0m in \u001b[0;36mget_ids\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0msignal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_signal_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnoise_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0msignal_noise_add\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevelop_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msignal_noise_add\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9e14a5df647e>\u001b[0m in \u001b[0;36mdevelop_data\u001b[0;34m(self, signal_path, noise_path)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0msignal_noise_add\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mixed_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_sr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSNR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform_stft\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0msignal_noise_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal_noise_add\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_fft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhop_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhop_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwin_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9e14a5df647e>\u001b[0m in \u001b[0;36mget_mixed_signal\u001b[0;34m(self, signal, noise, default_sr, sec, SNR)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mzero_signal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msig_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0msignal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_signal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0mzero_signal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0msignal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0msig_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 4.54 GiB (GPU 0; 7.80 GiB total capacity; 4.54 GiB already allocated; 2.38 GiB free; 4.54 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    noise_dir = \"./dataset/UrbanSound8K/audio/\"\n",
    "    signal_dir = \"./dataset/cv-corpus-5.1-2020-06-22-Resampled/en/clips/\"\n",
    "    signal_nums_save = \"./dataset_loader_files/signal_paths_nums_save.npy\"\n",
    "    num_noise_samples=100\n",
    "    num_signal_samples = 1000\n",
    "    noise_save_path = \"./dataset_loader_files/noise_paths_save.npy\"\n",
    "    n_fft=400\n",
    "    win_length=n_fft\n",
    "    hop_len=n_fft//4\n",
    "    create_specgram = False\n",
    "    perform_stft = False\n",
    "    default_sr = 16000\n",
    "    sec = 6\n",
    "    augment=True\n",
    "\n",
    "    signal_synthesis_dataset = Signal_Synthesis_DataGen(noise_dir, signal_dir, \\\n",
    "                    signal_nums_save=signal_nums_save, num_noise_samples=num_noise_samples, \\\n",
    "                    num_signal_samples=num_signal_samples, noise_path_save=noise_save_path, \\\n",
    "                     n_fft=n_fft, win_length=win_length, hop_len=hop_len, create_specgram=create_specgram, \\\n",
    "                     perform_stft=perform_stft, normalize=True, default_sr=default_sr, sec=sec, augment=False)\n",
    "    \n",
    "    signal_mix, signal = signal_synthesis_dataset.__getitem__(4532)\n",
    "    print(signal_mix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dir = \"./dataset/UrbanSound8K/all_files/\"\n",
    "signal_dir = \"./dataset/cv-corpus-5.1-2020-06-22/en/clips/\"\n",
    "num_samples=200, \n",
    "noise_path_save = \"./noise_paths_save.npy\"\n",
    "default_sr = 16000\n",
    "sec = 6\n",
    "augment=False\n",
    "\n",
    "signal_synthesis_dataset = Signal_Synthesis_DataGen(noise_dir, signal_dir, num_samples=200, noise_path_save=noise_path_save,\\\n",
    "                 n_fft=400, win_length=400, hop_len=200, f_min=0, f_max=8000, \\\n",
    "                 perform_stft=True, normalize=True, default_sr=16000, sec=6, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_mix, signal = signal_synthesis_dataset.__getitem__(4532)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(signal_mix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(signal_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(signal_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
