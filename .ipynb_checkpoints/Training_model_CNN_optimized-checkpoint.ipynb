{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# import librosa\n",
    "import torchaudio as ta\n",
    "ta.set_audio_backend(\"sox\")\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd.profiler as profiler\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import glob\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from python_files.Noise_Reduction_Datagen_paths import Signal_Synthesis_DataGen\n",
    "# from python_files.unet_basic import Model\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import gc\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "# from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "from fastai.data.core import DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.6'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fastai\n",
    "\n",
    "fastai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8004"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cudnn.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0a0+16f4b0e'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 2070 Super with Max-Q Design'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_default_dtype(torch.float16)\n",
    "# torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "# test_model = Model()\n",
    "\n",
    "# test_model(torch.rand(1, 1, 16384*10)).shape\n",
    "\n",
    "# torch.rand(32, 16384*10).unsqueeze(dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(dir_path):\n",
    "    paths = []\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        for name in files:\n",
    "            if name.endswith(\".wav\") or name.endswith(\".mp3\"):\n",
    "                paths.append(os.path.join(root, name))\n",
    "                \n",
    "    paths = np.asarray(paths)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_paths = np.load(\"./dataset_loader_files/signal_paths_nums_save.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_paths_sh = get_paths(\"./dataset/Reduced_noise/\")\n",
    "shuffle(noise_paths_sh)\n",
    "\n",
    "signal_paths_sh = get_paths(\"./dataset/Reduced_clean_signals/\")\n",
    "shuffle(signal_paths_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['./dataset/Reduced_clean_signals/common_voice_en_22017677.mp3',\n",
       "       './dataset/Reduced_clean_signals/common_voice_en_18662002.mp3',\n",
       "       './dataset/Reduced_clean_signals/common_voice_en_21683790.mp3',\n",
       "       ...,\n",
       "       './dataset/Reduced_clean_signals/common_voice_en_22207403.mp3',\n",
       "       './dataset/Reduced_clean_signals/common_voice_en_19800128.mp3',\n",
       "       './dataset/Reduced_clean_signals/common_voice_en_21307739.mp3'],\n",
       "      dtype='<U60')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_paths_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_paths = np.load(\"./dataset_loader_files/signal_paths_nums_save.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108155 ./dataset/Reduced_clean_signals/common_voice_en_18662002.mp3\n"
     ]
    }
   ],
   "source": [
    "noise_paths = noise_paths_sh[:-1000]\n",
    "signal_paths = signal_paths_sh[:-1000]\n",
    "signal_dir = \"\"#\"./dataset/cv-corpus-5.1-2020-06-22-Resampled/en/clips\"\n",
    "noise_save_path = \"\"#\"./dataset_loader_files/noise_paths_resampled_save.npy\"\n",
    "train = True\n",
    "n_fft=1024\n",
    "win_length=n_fft\n",
    "hop_len=n_fft//4\n",
    "create_specgram = False\n",
    "perform_stft = False\n",
    "default_sr = 16000\n",
    "sec = (16384/default_sr)*2\n",
    "augment=True\n",
    "device_datagen = \"cpu\"\n",
    "\n",
    "train_ds = Signal_Synthesis_DataGen(noise_paths, signal_paths, signal_dir, \\\n",
    "                 n_fft=n_fft, win_length=win_length, hop_len=hop_len, create_specgram=create_specgram, \\\n",
    "                 perform_stft=perform_stft, normalize=True, default_sr=default_sr, sec=sec, epsilon=1e-5, augment=False, device=device_datagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 ./dataset/Reduced_clean_signals/common_voice_en_18974170.mp3\n"
     ]
    }
   ],
   "source": [
    "noise_paths = noise_paths_sh[-1000:]\n",
    "signal_paths = signal_paths_sh[-1000:]\n",
    "signal_dir = \"\"#\"./dataset/cv-corpus-5.1-2020-06-22-Resampled/en/clips\"\n",
    "noise_save_path = \"\"#\"./dataset_loader_files/noise_paths_resampled_save.npy\"\n",
    "train = False\n",
    "\n",
    "val_ds = Signal_Synthesis_DataGen(noise_paths, signal_paths, signal_dir,\\\n",
    "                 n_fft=n_fft, win_length=win_length, hop_len=hop_len, create_specgram=create_specgram, \\\n",
    "                 perform_stft=perform_stft, normalize=True, default_sr=default_sr, sec=sec, epsilon=1e-5, augment=False, device=device_datagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4014, 0.4044, 0.3798,  ..., 0.3980, 0.4770, 0.4289]]),\n",
       " tensor([[0.3167, 0.3267, 0.3127,  ..., 0.4212, 0.4208, 0.4210]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BATCH_SIZE = 104\n",
    "shuffle = True\n",
    "num_workers = 0\n",
    "pin_memory = False\n",
    "\n",
    "# data_loader = DataLoader(signal_synthesis_dataset, batch_size=BATCH_SIZE, shuffle=shuffle, num_workers=num_workers)\n",
    "# data_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=shuffle, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "dls = DataLoaders.from_dsets(train_ds, val_ds, bs=BATCH_SIZE, num_workers=num_workers, pin_memory=pin_memory).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unsqueezer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Unsqueezer, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.usqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.after_batch = Unsqueezer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for data in dls.train:\n",
    "#     print(data[0].max(), data[0].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80142855"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # data_loader_iter = iter(data_loader)\n",
    "# for index, i in enumerate(data_loader):\n",
    "# #     i = next(data_loader)\n",
    "#     if index < 32-1:\n",
    "#         pass\n",
    "#     else:\n",
    "#         break\n",
    "#     print(i[1].shape,i[0].min(), i[0].max(), i[0].dtype, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# stft_sig = torch.stft(i[0], n_fft=n_fft, hop_length=hop_len, win_length=win_length)\n",
    "# istft_sig = torch.istft(stft_sig, n_fft=n_fft, hop_length=hop_len, win_length=win_length)\n",
    "# i[1].max()\n",
    "\n",
    "# nan_sig = signal_synthesis_dataset.__getitem__(119)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_sig[0].max(), nan_sig[1].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stft_sig.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stft_sig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# istft_sig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize(tensor):\n",
    "#     tensor_minusmean = tensor - tensor.min()\n",
    "#     return tensor_minusmean/tensor_minusmean.abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aud = i[0][0]\n",
    "\n",
    "# aud.dtype\n",
    "\n",
    "# aud.max(), aud.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = aud.t().to(\"cpu\").numpy()\n",
    "# ipd.Audio(x, rate=default_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sig1 = i[0].unsqueeze(dim=1)\n",
    "# sig2 = i[0].unsqueeze(dim=1)\n",
    "# stacked_sig = torch.cat((sig1, sig2), dim=1)\n",
    "\n",
    "# sig2 = i[0].unsqueeze(dim=1)\n",
    "# sig2.shape\n",
    "\n",
    "# torch.sum(stacked_sig, dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.floor(((default_sr*sec) - (win_length - 1) - 1)/ hop_len + 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_fft // 2 + 1\n",
    "\n",
    "# n_fft // 2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stft_sig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Negative_SNR_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Negative_SNR_Loss, self).__init__()\n",
    "    \n",
    "    def forward(self, sig_pred, sig_true):\n",
    "        \n",
    "        sig_true_sq = torch.square(sig_true)\n",
    "        sig_pred_sq = torch.square(sig_true - sig_pred)\n",
    "\n",
    "        sig_true_mean = torch.mean(sig_true_sq)\n",
    "        sig_pred_mean = torch.mean(sig_pred_sq)\n",
    "\n",
    "        snr = sig_true_mean / sig_pred_mean + 1e-7\n",
    "        loss = -1*torch.log10(snr)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mod_MSELoss(nn.Module):\n",
    "    def __init__(self, mul_factor):\n",
    "        super(Mod_MSELoss, self).__init__()\n",
    "        self.loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "        self.mul_factor = mul_factor\n",
    "        \n",
    "        \n",
    "    def forward(self, sig_pred, sig_true):\n",
    "        loss = self.loss_fn(sig_pred, sig_true)\n",
    "        loss = self.mul_factor*loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class InstantLayerNormalization(nn.Module):\n",
    "#     def __init__(self, in_shape, out_shape):\n",
    "#         self.in_shape = in_shape\n",
    "#         self.out_shape = out_shape\n",
    "        \n",
    "#         self.epsilon = 1e-7\n",
    "#         self.gamma = None\n",
    "#         self.beta = None\n",
    "        \n",
    "#         super(InstantLayerNormalization, self).__init__()\n",
    "        \n",
    "#         self.gamma = torch.ones(out_shape)\n",
    "#         self.gamma = nn.Parameter(self.gamma)\n",
    "        \n",
    "#         self.beta = torch.zeros(out_shape)\n",
    "#         self.beta = nn.Parameter(self.beta)\n",
    "        \n",
    "#     def forward(self, inps):\n",
    "#         mean = inps.mean(-1, keepdim=True)\n",
    "#         variance = torch.mean(torch.square(inps - mean), dim=-1, keepdim=True)\n",
    "#         std = torch.sqrt(variance + self.epsilon)\n",
    "        \n",
    "#         outs = (inps - mean) / std\n",
    "#         print(outs.shape, self.gamma.shape)\n",
    "#         outs = outs * self.gamma\n",
    "#         outs = outs + self.beta\n",
    "        \n",
    "#         return outs\n",
    "    \n",
    "# class Multiply():\n",
    "#     def __init__(self):\n",
    "#         super(Multiply, self).__init__()\n",
    "    \n",
    "#     def forward(self, ten1, ten2):\n",
    "#         mul_out = torch.mul(ten1, ten2)\n",
    "#         return mul_out\n",
    "\n",
    "# class NoiseReducer(pl.LightningModule):\n",
    "#     def __init__(self, default_sr, n_fft, win_length, hop_len, sec, dropout=0.5, batch_first=True, stride=2, normalized=False, bidir=False):\n",
    "        \n",
    "#         self.default_sr = default_sr\n",
    "#         self.n_fft = n_fft\n",
    "#         self.win_length = win_length\n",
    "#         self.hop_len = hop_len\n",
    "#         self.sec = sec\n",
    "#         self.normalized = normalized\n",
    "        \n",
    "#         self.conv_filters = 512\n",
    "        \n",
    "#         # Universal LSTM Units\n",
    "#         self.batch_first = True\n",
    "#         self.dropout = 0.25\n",
    "#         self.bidir = bidir\n",
    "#         self.lstm_prelu_ins = int(np.floor(((default_sr*sec) - (win_length - 1) - 1)/ hop_len + 5))\n",
    "        \n",
    "#         # LSTM 1 UNITS\n",
    "#         self.rnn1_dims = n_fft // 2 + 1\n",
    "#         self.hidden_size_1 = 256\n",
    "#         self.num_layers = 2\n",
    "       \n",
    "        \n",
    "#         # LSTM 2 UNITS\n",
    "#         self.rnn2_dims = self.conv_filters\n",
    "#         self.hidden_size_2 = self.hidden_size_1\n",
    "        \n",
    "#         # Conv1d Layer Units\n",
    "#         self.conv1_in = 1\n",
    "#         self.conv1_out = self.conv_filters\n",
    "        \n",
    "        \n",
    "        \n",
    "#         # InstanceNorm Layer Units\n",
    "#         self.instance1_in = self.rnn1_dims\n",
    "#         self.instance2_in = self.conv1_out\n",
    "        \n",
    "#         # Dense1 Layer Units\n",
    "#         self.dense1_in = self.hidden_size_1\n",
    "#         self.dense1_out = self.rnn1_dims #int(np.floor(((default_sr*sec) - (win_length - 1) - 1)/ hop_len + 5))#3))\n",
    "        \n",
    "#         # Dense2 Layer Units\n",
    "#         self.dense2_in = self.hidden_size_2\n",
    "#         self.dense2_out = self.conv1_out\n",
    "        \n",
    "#         # Dense3 Layer Units\n",
    "#         self.dense3_in = self.hidden_size_1\n",
    "#         self.dense3_out = self.rnn1_dims\n",
    "        \n",
    "#         # Conv2d Layer Units\n",
    "#         self.conv2_in = self.dense2_out\n",
    "#         self.conv2_out = self.conv_filters\n",
    "        \n",
    "\n",
    "#         super(NoiseReducer, self).__init__()\n",
    "        \n",
    "#         self.loss_fn = Mod_MSELoss(mul_factor=1000)\n",
    "        \n",
    "#         self.lstm1 = nn.LSTM(input_size=self.rnn1_dims, hidden_size=self.hidden_size_1, num_layers=self.num_layers, batch_first=self.batch_first, dropout=self.dropout, bidirectional=self.bidir)\n",
    "#         self.lstm3 = nn.LSTM(input_size=self.rnn1_dims, hidden_size=self.hidden_size_1, num_layers=self.num_layers, batch_first=self.batch_first, dropout=self.dropout, bidirectional=self.bidir)\n",
    "# #         self.lstm1_1 = nn.LSTMCell(input_size=self.rnn1_dims, hidden_size==self.hidden_size_1, )\n",
    "        \n",
    "#         print(self.rnn2_dims)\n",
    "#         self.lstm2 = nn.LSTM(input_size=self.rnn2_dims, hidden_size=self.hidden_size_2, num_layers=self.num_layers*2, batch_first=self.batch_first, dropout=self.dropout, bidirectional=self.bidir)\n",
    "        \n",
    "        \n",
    "#         self.instancenorm1 = nn.InstanceNorm1d(self.rnn1_dims)\n",
    "#         self.instancenorm2 = nn.InstanceNorm1d(self.rnn2_dims)\n",
    "#         self.instancenorm3 = nn.InstanceNorm1d(self.rnn1_dims)\n",
    "        \n",
    "#         self.dense1 = nn.Linear(self.dense1_in, self.dense1_out)\n",
    "#         self.dense2 = nn.Linear(self.dense2_in, self.dense2_out)\n",
    "#         self.dense3 = nn.Linear(self.dense3_in, self.dense3_out)\n",
    "        \n",
    "#         self.conv1 = nn.Conv1d(self.conv1_in, self.conv1_out, kernel_size=3, stride=1, padding=1)\n",
    "#         self.conv2 = nn.Conv1d(self.conv2_in, self.conv2_out, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "#         self.prelu_conv1 = nn.PReLU(self.conv1_out)\n",
    "#         self.prelu_conv2 = nn.PReLU(self.conv2_out)\n",
    "        \n",
    "#         self.prelu_lstm1 = nn.PReLU(self.lstm_prelu_ins)\n",
    "#         self.prelu_lstm3 = nn.PReLU(self.lstm_prelu_ins)\n",
    "#         self.prelu_lstm2 = nn.PReLU(self.hidden_size_2)\n",
    "        \n",
    "#     @torch.jit.export\n",
    "#     def stft_layer(self, sig):\n",
    "        \n",
    "#         sig_stft = torch.stft(sig, n_fft=self.n_fft, hop_length=self.hop_len, win_length=self.win_length)\n",
    "        \n",
    "#         sig_cplx = torch.view_as_complex(sig_stft)\n",
    "#         mag = sig_cplx.abs().permute(0, 2, 1)\n",
    "#         angle = sig_cplx.angle().permute(0, 2, 1)\n",
    "\n",
    "# #         mag = sig_stft[:,:,:,0].permute(0, 2, 1)\n",
    "# #         angle = sig_stft[:,:,:,1].permute(0, 2, 1)\n",
    "        \n",
    "#         return [mag, angle]\n",
    "    \n",
    "#     @torch.jit.export\n",
    "#     def istft_layer(self, mag, angle):\n",
    "#         mag = mag.permute(0, 2, 1)\n",
    "#         angle = angle.permute(0, 2, 1)\n",
    "#         mag = torch.unsqueeze(mag, dim=-1)\n",
    "#         angle = torch.unsqueeze(angle, dim=-1)\n",
    "#         pre_stft = torch.cat((mag, angle), dim=-1)\n",
    "#         stft_sig = torch.istft(pre_stft, n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_len)\n",
    "        \n",
    "#         return stft_sig\n",
    "    \n",
    "#     @torch.cuda.amp.autocast()\n",
    "#     def forward(self, inp_tensor):\n",
    "        \n",
    "#         mag, angle = self.stft_layer(inp_tensor)\n",
    "#         mag_norm = self.instancenorm1(mag)\n",
    "#         angle_norm = self.instancenorm3(angle)\n",
    "        \n",
    "#         x_mag, hidden_states_mag = self.lstm1(mag_norm)\n",
    "#         x_angle, hidden_states_angle = self.lstm3(angle_norm)\n",
    "\n",
    "#         mask_mag = F.relu(self.dense1(x_mag))\n",
    "#         estimated_mag = torch.mul(mag, mask_mag)\n",
    "        \n",
    "#         mask_angle = F.relu(self.dense3(x_angle))\n",
    "#         estimated_angle = torch.mul(angle, mask_angle)\n",
    "        \n",
    "#         signal = self.istft_layer(estimated_mag, estimated_angle)\n",
    "#         signal = signal.unsqueeze(dim=1)\n",
    "\n",
    "#         feature_rep = self.conv1(signal)\n",
    "#         feature_rep = self.prelu_conv2(feature_rep)\n",
    "        \n",
    "#         feature_norm = self.instancenorm2(feature_rep)\n",
    "#         feature_norm = feature_norm.permute(0, 2, 1)\n",
    "#         x, hidden_states = self.lstm2(feature_norm)\n",
    "#         mask = self.dense2(x)\n",
    "#         feature_mask = F.relu(mask)\n",
    "#         feature_mask = feature_mask.permute(0, 2, 1)\n",
    "\n",
    "#         estimate_feat = torch.mul(feature_rep, feature_mask)\n",
    "        \n",
    "#         estimate_frames = (self.conv2(estimate_feat))\n",
    "#         estimate_frames = self.prelu_conv2(estimate_frames)\n",
    "#         estimate_sig = torch.sum(estimate_frames, dim=1)\n",
    "        \n",
    "#         return estimate_sig\n",
    "    \n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         x, y = batch\n",
    "#         y_pred = self(x.unsqueeze(dim=1).to(device))\n",
    "#         loss = self.loss_fn(y_pred, y.unsqueeze(dim=1))\n",
    "\n",
    "#         return loss\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         x, y = batch\n",
    "#         y_pred = self(x.to(device))\n",
    "        \n",
    "#         loss = self.loss_fn(y_pred, y)\n",
    "#         self.log(\"val_loss\", loss, enable_graph=True, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "#     def configure_optimizers(self):\n",
    "#         return torch.optim.AdamW(self.parameters(), lr=0.001, weight_decay=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSamplingLayer(nn.Module):\n",
    "    def __init__(self, channel_in, channel_out, dilation=1, kernel_size=15, stride=1, padding=7):\n",
    "        super(DownSamplingLayer, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv1d(channel_in, channel_out, kernel_size=kernel_size,\n",
    "                      stride=stride, padding=padding, dilation=dilation),\n",
    "            nn.BatchNorm1d(channel_out),\n",
    "            nn.LeakyReLU(negative_slope=0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, ipt):\n",
    "        return self.main(ipt)\n",
    "\n",
    "class UpSamplingLayer(nn.Module):\n",
    "    def __init__(self, channel_in, channel_out, kernel_size=5, stride=1, padding=2):\n",
    "        super(UpSamplingLayer, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv1d(channel_in, channel_out, kernel_size=kernel_size,\n",
    "                      stride=stride, padding=padding),\n",
    "            nn.BatchNorm1d(channel_out),\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, ipt):\n",
    "        return self.main(ipt)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_layers=12, channels_interval=24):\n",
    "        super(Model, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.channels_interval = channels_interval\n",
    "        encoder_in_channels_list = [1] + [i * self.channels_interval for i in range(1, self.n_layers)]\n",
    "        encoder_out_channels_list = [i * self.channels_interval for i in range(1, self.n_layers + 1)]\n",
    "\n",
    "        #          1    => 2    => 3    => 4    => 5    => 6   => 7   => 8   => 9  => 10 => 11 =>12\n",
    "        # 16384 => 8192 => 4096 => 2048 => 1024 => 512 => 256 => 128 => 64 => 32 => 16 =>  8 => 4\n",
    "        self.encoder = nn.ModuleList()\n",
    "        for i in range(self.n_layers):\n",
    "            self.encoder.append(\n",
    "                DownSamplingLayer(\n",
    "                    channel_in=encoder_in_channels_list[i],\n",
    "                    channel_out=encoder_out_channels_list[i]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv1d(self.n_layers * self.channels_interval, self.n_layers * self.channels_interval, 15, stride=1,\n",
    "                      padding=7),\n",
    "            nn.BatchNorm1d(self.n_layers * self.channels_interval),\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        )\n",
    "\n",
    "        decoder_in_channels_list = [(2 * i + 1) * self.channels_interval for i in range(1, self.n_layers)] + [\n",
    "            2 * self.n_layers * self.channels_interval]\n",
    "        decoder_in_channels_list = decoder_in_channels_list[::-1]\n",
    "        decoder_out_channels_list = encoder_out_channels_list[::-1]\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for i in range(self.n_layers):\n",
    "            self.decoder.append(\n",
    "                UpSamplingLayer(\n",
    "                    channel_in=decoder_in_channels_list[i],\n",
    "                    channel_out=decoder_out_channels_list[i]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv1d(1 + self.channels_interval, 1, kernel_size=1, stride=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        tmp = []\n",
    "        o = input\n",
    "\n",
    "        # Up Sampling\n",
    "        for i in range(self.n_layers):\n",
    "            o = self.encoder[i](o)\n",
    "#             print(o.shape)\n",
    "            tmp.append(o)\n",
    "            # [batch_size, T // 2, channels]\n",
    "            o = o[:, :, ::2]\n",
    "\n",
    "        o = self.middle(o)\n",
    "\n",
    "        # Down Sampling\n",
    "        for i in range(self.n_layers):\n",
    "            # [batch_size, T * 2, channels]\n",
    "            o = F.interpolate(o, scale_factor=2, mode=\"linear\", align_corners=True)\n",
    "            # Skip Connection\n",
    "            # print(o.shape, len(tmp))\n",
    "            o = torch.cat([o, tmp[self.n_layers - i -1]], dim=1)\n",
    "            o = self.decoder[i](o)\n",
    "\n",
    "        o = torch.cat([o, input], dim=1)\n",
    "        o = self.out(o)\n",
    "        return o\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         x, y = batch\n",
    "#         y_pred = self(x.unsqueeze(dim=1).to(device))\n",
    "#         loss = self.loss_fn(y_pred, y.unsqueeze(dim=1))\n",
    "\n",
    "#         return loss\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         x, y = batch\n",
    "#         y_pred = self(x.unsqueeze(dim=1).to(device))\n",
    "        \n",
    "#         loss = self.loss_fn(y_pred, y.unsqueeze(dim=1))\n",
    "#         self.log(\"val_loss\", loss, enable_graph=True, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "#     def configure_optimizers(self):\n",
    "#         return torch.optim.AdamW(self.parameters(), lr=0.001, weight_decay=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Primary model\n"
     ]
    }
   ],
   "source": [
    "use_scripted_model = False\n",
    "w_decay = 1e-4\n",
    "\n",
    "if not use_scripted_model:\n",
    "    print(\"Using Primary model\")\n",
    "    model = Model(12, 24)#NoiseReducer(default_sr=default_sr, n_fft=n_fft, win_length=win_length, hop_len=hop_len, sec=sec).to(device)\n",
    "    model.to(device)\n",
    "else:\n",
    "    print(\"Using Scripted Model\")\n",
    "    model = torch.jit.script(Model())\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0)\n",
    "criterion = Mod_MSELoss(mul_factor=1000)\n",
    "n_epochs=100\n",
    "\n",
    "\n",
    "model.train()\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, model, loss_func=criterion, model_dir=\"./Model_saves/\", \\\n",
    "                cbs=[ShowGraphCallback(), SaveModelCallback(monitor='valid_loss', fname='bestmodel', every_epoch=True)]).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.load(\"cnn_model_best_1_78\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr = 0.33113112449646#0.19054607152938843"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.load(\"cnn_model_best_1_78\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/20 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='4163' class='' max='770604' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.54% [4163/770604 1:57:10<359:33:29 1.3296]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(\"learner_cnn_save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = pl.callbacks.ModelCheckpoint(dirpath=\"Model_saves/\", monitor=\"val_loss\", mode=\"min\", verbose=True, filename=\"lightnng_save_lstm.pt\")\n",
    "\n",
    "# trainer = pl.Trainer(gpus=[0],max_epochs=20, \\\n",
    "#                     precision=16, \\\n",
    "#                     amp_backend=\"native\",\\\n",
    "#                     num_sanity_val_steps=10, \\\n",
    "#                     benchmark=True, reload_dataloaders_every_epoch=True, \\\n",
    "#                     weights_save_path=\"./Model_saves/\", weights_summary=\"top\", \\\n",
    "#                     profiler=\"simple\",  callbacks=[checkpoint])\n",
    "\n",
    "\n",
    "# lr_finder = trainer.tuner.lr_find(model, dls.train, dls.valid)\n",
    "\n",
    "# fig = lr_finder.plot(suggest=True); fig.show()\n",
    "\n",
    "# lr_finder.suggestion()\n",
    "\n",
    "# trainer.fit(model, dls.train, dls.valid)x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/./././"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake_inputs = torch.randn(BATCH_SIZE, int(default_sr*sec)).type(torch.float32).to(device)\n",
    "# outs = model(fake_inputs)\n",
    "# outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./Model_saves/Pytorch_model_2_save_LSTM_512_filters.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    \n",
    "    loop = tqdm(enumerate(data_loader), leave=True, total=len(data_loader))\n",
    "    train_loss = np.zeros((len(data_loader)))\n",
    "    loop.set_description(f\"Epoch: [ {epoch}/{n_epochs} ]\\t\")\n",
    "\n",
    "    \n",
    "    for index, (data, target) in loop:\n",
    "        \n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        for group in optimizer.param_groups:\n",
    "            for param in group[\"params\"]:\n",
    "                param.data = param.data.add(-w_decay * group[\"lr\"], param.data)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "#         output = model(data)\n",
    "#         loss = criterion(output, target)\n",
    "        \n",
    "#         loss.backward()\n",
    "#         for group in optimizer.param_groups:\n",
    "#             for param in group[\"params\"]:\n",
    "#                 param.data = param.data.add(-w_decay * group[\"lr\"], param.data)\n",
    "#         optimizer.step()\n",
    "\n",
    "        train_loss[index] = loss.item()\n",
    "        if np.isnan(loss.item()) or np.isnan(np.sum(train_loss)/index+1e-5):\n",
    "            print(f\"Data shape = {data.shape}\\nTarget Shape = {target.shape}, \\nindex = {index}\")\n",
    "        disp_loss = np.sum(train_loss)/index+1e-5\n",
    "        loop.set_postfix(loss = disp_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.closure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./Model_saves/Pytorch_model_2_save_LSTM_512_filters.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "noise_add_sig, main_sig = val_ds.__getitem__(2345)\n",
    "noise_add_sig = torch.unsqueeze(noise_add_sig, dim=0).to(device)\n",
    "main_sig = torch.unsqueeze(main_sig, dim=0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outs = learn.model(noise_add_sig).squeeze()\n",
    "\n",
    "outs.shape\n",
    "\n",
    "x = outs.t().to(\"cpu\").numpy()\n",
    "ipd.Audio(x, rate=default_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs.squeeze().t().to(\"cpu\").numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = noise_add_sig.squeeze().t().to(\"cpu\").numpy()\n",
    "ipd.Audio(x, rate=default_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = main_sig.squeeze().t().to(\"cpu\").numpy()\n",
    "ipd.Audio(x, rate=default_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_add_sig.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
